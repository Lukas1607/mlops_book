{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Apache Airflow\n",
    "\n",
    "Innerhalb des MLOps-Projekts wurde auch Apache Airflow zur Automatisierung von ausgewählten Arbeitsschritten verwendet. <br><br>\n",
    "Zur Demonstration von Apache Airflow wurde der erste Teil (1.) \"Data Correction\" automatisiert. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 Vorgehen\n",
    "\n",
    "Das Vorgehen zur Integration von Apache Airflow lässt sich grundsätzlich in folgende drei Schritte aufteilen:\n",
    "\n",
    "1. Installation von Apache Airflow\n",
    "2. Erstellung der Ordnerstruktur\n",
    "3. Definition der Tasks & Skripte\n",
    "\n",
    "Im ersten Schritt wurde dabei Apache Airflow installiert. <br>Im anschließenden Schritt wurde die Ordner- und Pfadstruktur festgelegt. Dabei wurde ein Ordner(\"dags\") erstellt in welchem sich die Datei \"airflow_mlops.py\" befindet. Innerhalb des \"dags\"-Ordners ist dabei ein weiterer Unterordner (\"airflow-dag-reg\") hinterlegt. Innerhalb des Unterordners werden die Skripte hinterlegt, welche zur Automatisierung ausgeführt werden sollen. <br><br>\n",
    "Innerhalb der Definition der Tasks & Skripte wurde die Datei \"airflow_mlops.py\" erstellt. Innerhalb des Dokuments können die jeweiligen Schritte, welche automatisiert werden sollen, definiert werden. In diesem Beispiel wurde nur eine Task (\"t1\") zur Automatisierung der Data Correction definiert. Dementsprechend ist nur ein Skript zur Ausführung der Task(s) hinterlegt, \"data_prep.py\". Durch die Datei \"data_prep.py\" werden die verschiedenen Schritte zur Data Correction mit Pandas durchgeführt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 Dateiübersicht\n",
    "\n",
    "## Airflow_mlops.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------\n",
    "# SETUP\n",
    "\n",
    "# The DAG object; we'll need this to instantiate a DAG\n",
    "from airflow import DAG\n",
    "\n",
    "# Operators; we need this to operate!\n",
    "from airflow.operators.bash import BashOperator\n",
    "\n",
    "# Module for manipulating dates and times\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "    # To change timezones, use Pendulum https://pendulum.eustace.io/\n",
    "\n",
    "# Some convenience functions\n",
    "from textwrap import dedent\n",
    "\n",
    "#---------------------------------------\n",
    "# DEFAULT DAG ARGUMENTS\n",
    "\n",
    "with DAG(\n",
    "    # the following string is the unique identifier for your DAG\n",
    "    'airflow_mlops', \n",
    "    # These args will get passed on to each operator\n",
    "    # You can override them on a per-task basis during operator initialization\n",
    "    default_args={\n",
    "        'depends_on_past': True,\n",
    "        'email': ['my-email@example.com'],\n",
    "        'email_on_failure': False,\n",
    "        'email_on_retry': False,\n",
    "        'retries': 1,\n",
    "        'retry_delay': timedelta(minutes=5),\n",
    "        # 'queue': 'bash_queue',\n",
    "        # 'pool': 'backfill',\n",
    "        # 'priority_weight': 10,\n",
    "        # 'end_date': datetime(2022, 6, 1),\n",
    "        # 'wait_for_downstream': False,\n",
    "        # 'sla': timedelta(hours=2),\n",
    "        # 'execution_timeout': timedelta(seconds=300),\n",
    "        # 'on_failure_callback': some_function,\n",
    "        # 'on_success_callback': some_other_function,\n",
    "        # 'on_retry_callback': another_function,\n",
    "        # 'sla_miss_callback': yet_another_function,\n",
    "        # 'trigger_rule': 'all_success'\n",
    "    },\n",
    "    description='Automatisierung der Datacorrection',\n",
    "    schedule_interval=timedelta(days=1),\n",
    "    start_date=datetime(2022, 1, 1),\n",
    "    catchup=False,\n",
    "    tags=['mlops'],\n",
    "\n",
    ") as dag:\n",
    "\n",
    "\n",
    "    #---------------------------------------\n",
    "    # DEFINE OPERATERS\n",
    "    \n",
    "    t1 = BashOperator(\n",
    "        depends_on_past=False,\n",
    "        task_id='data_prep',\n",
    "        bash_command='python /Users/lukas/airflow/dags/airflow-dag-reg/data_prep.py',\n",
    "    )                      \n",
    "\n",
    "    # Task documentation\n",
    "    t1.doc_md = dedent(\n",
    "        \"\"\"\\\n",
    "    #### Task Documentation\n",
    "    \n",
    "    Data preparation\n",
    "\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    # Hier könnten weitere tasks definiert werden (Beispiel Modellerstellung und Training)\n",
    "    # t2 = ....\n",
    "\n",
    "    #----------------\n",
    "    # SETTING UP DEPENDENCIES \n",
    "    # Ausführung der definierten Tasks (t1) -> hier könnten weitere Tasks ausgeführt werden (Beispiel t2...)\n",
    "\n",
    "    t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sobald die Task (t1) ausgeführt wird startet das Skript \"data_prep.py\" und führt die Data Correction durch Anschließend wird der Datensatz lokal auf dem angegebenen Pfad(Desktop) gespeichert.\n",
    "\n",
    "## Data_prep.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    Data preparation for Simulation for Data Science Products: Used car prices\n",
    "\n",
    "Step 1) Import data (Used car prices) with pandas\n",
    "Step 2) Data corrections\n",
    "Step 3) Save data as csv to local folder\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#------------------------------------------------------\n",
    "# Setup\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from my_path import home_path, airflow_path\n",
    "\n",
    "#------------------------------------------------------\n",
    "# Import Data\n",
    "\n",
    "df = pd.read_csv(\"/Users/lukas/Documents/MLOPS/PL_2/car_prices.csv\", error_bad_lines=False,warn_bad_lines=True)\n",
    "\n",
    "#------------------------------------------------------\n",
    "# Data correction\n",
    "\n",
    "# Rename columns \n",
    "df = df.rename(columns={\"make\":\"brand\",\"body\":\"type\",\"sellingprice\":\"price\",\"saledate\":\"date\"})\n",
    "\n",
    "# Drop irrelevant columns for this project\n",
    "df = df.drop(['vin','mmr'],axis=1)\n",
    "\n",
    "# Drop missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Lower text to avoid duplicates\n",
    "df.brand = df.brand.str.lower()\n",
    "df.model = df.model.str.lower()\n",
    "df.type = df.type.str.lower()\n",
    "\n",
    "#------------------------------------------------------\n",
    "#Export Data to CSV\n",
    "\n",
    "df.to_csv(r'/Users/lukas/Desktop/car_price_airflow.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
